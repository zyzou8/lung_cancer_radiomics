{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contralateral Model Pipeline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import random\n",
    "from DicomRTTool.ReaderWriter import DicomReaderWriter, ROIAssociationClass\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, TensorDataset\n",
    "import pickle\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train/dev/test split lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/lam3654/MSAI_pneumonitis/lung_cancer_radiomics/train_dev_test_lists'\n",
    "\n",
    "with open(os.path.join(path, \"small_train_data.json\"), \"r\") as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open(os.path.join(path, \"small_train_labels.json\"), \"r\") as file:\n",
    "    train_labels = json.load(file)\n",
    "    \n",
    "with open(os.path.join(path, \"small_dev_data.json\"), \"r\") as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open(os.path.join(path, \"small_dev_labels.json\"), \"r\") as file:\n",
    "    train_labels = json.load(file)\n",
    "\n",
    "with open(os.path.join(path, \"small_test_data.json\"), \"r\") as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open(os.path.join(path, \"small_test_labels.json\"), \"r\") as file:\n",
    "    train_labels = json.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set number of skipped slices\n",
    "skip = 4\n",
    "\n",
    "left_contours = ['l lung', 'tru left lung', 'lt lung 2', 'lt lung', 'lung_l', 'l_lung', 'left lung']\n",
    "right_contours = ['new right lung', 'r_lung', 'rt lung', 'lung_r', 'r lung', 'right lung']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lung_side(path_name):\n",
    "\n",
    "    # Load the Excel spreadsheet into a pandas DataFrame\n",
    "    df = pd.read_excel('/home/lam3654/MSAI_pneumonitis/total_labels.xlsx')\n",
    "\n",
    "    # Define the unique column and the column to retrieve values from\n",
    "    unique_column = 'anon_id'\n",
    "    retrieve_column = 'side'\n",
    "\n",
    "    # Prompt the user for the unique column value to search\n",
    "    split_values = path_name.split('/')\n",
    "    search_value = split_values[-1]\n",
    "\n",
    "    # Find the rows matching the search value in the unique column\n",
    "    matching_rows = df[df[unique_column] == search_value]\n",
    "\n",
    "    # Retrieve the corresponding values from the retrieve column\n",
    "    side_values = matching_rows[retrieve_column]\n",
    "\n",
    "    if 'R' in side_values:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_contours = []\n",
    "skipped_cts = []\n",
    "total_arrays = []\n",
    "total_labels = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    try:\n",
    "        single_arrays = []\n",
    "        Dicom_path = train_data[i]\n",
    "        Dicom_reader = DicomReaderWriter(description='Examples', arg_max=True)\n",
    "        Dicom_reader.walk_through_folders(Dicom_path) \n",
    "        # all_rois = Dicom_reader.return_rois(print_rois=True) # Return a list of all rois present\n",
    "\n",
    "        right_side = lung_side(Dicom_path)\n",
    "        if right_side == False:\n",
    "            Contour_names = ['rlung'] \n",
    "            associations = [ROIAssociationClass('rlung', right_contours)]\n",
    "        else:\n",
    "            Contour_names = ['llung'] \n",
    "            associations = [ROIAssociationClass('llung', left_contours)]\n",
    "        Dicom_reader.set_contour_names_and_associations(contour_names=Contour_names, associations=associations)\n",
    "        indexes = Dicom_reader.which_indexes_have_all_rois()\n",
    "        if indexes != []:\n",
    "            pt_indx = indexes[-1]\n",
    "            Dicom_reader.set_index(pt_indx) \n",
    "            Dicom_reader.get_images_and_mask()  \n",
    "\n",
    "            image = Dicom_reader.ArrayDicom # image array\n",
    "            mask = Dicom_reader.mask # mask array\n",
    "\n",
    "            slice_locations = np.unique(np.where(mask != 0)[0]) # get indexes for where there is a contour present \n",
    "            slice_start = slice_locations[0] # first slice of contour \n",
    "            slice_end = slice_locations[len(slice_locations)-1] # last slice of contour\n",
    "\n",
    "            counter = 1\n",
    "\n",
    "            for img_arr, contour_arr in zip(image[slice_start:slice_end+1], mask[slice_start:slice_end+1]): \n",
    "                if counter % skip == 0: # if current slice is divisible by desired skip amount \n",
    "                    select = np.multiply(img_arr, contour_arr)\n",
    "                    single_arrays.append(select)\n",
    "                counter += 1\n",
    "\n",
    "            single_labels = [train_labels[i] for x in range(len(single_arrays))]\n",
    "            total_arrays = total_arrays + single_arrays\n",
    "            total_labels = total_labels + single_labels\n",
    "        else:\n",
    "            missed_contours.append(Dicom_path)\n",
    "    \n",
    "    except TypeError:\n",
    "        print(\"skip this dataset\")\n",
    "        skipped_cts.append(Dicom_path)\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save array and label lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_folder_path = '/home/lam3654/MSAI_pneumonitis/data/pneumonitis_np'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(np_folder_path, \"ipsi_train_arrays.npy\"), total_arrays)\n",
    "\n",
    "with open(os.path.join(np_folder_path, \"ipsi_train_labels_np.json\"), \"w\") as file:\n",
    "    json.dump(total_labels, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array_list = np.load(os.path.join(np_folder_path, \"ipsi_train_arrays.npy\"), allow_pickle=True)\n",
    "\n",
    "\n",
    "with open(os.path.join(np_folder_path, \"ipsi_train_labels_np.json\"), \"r\") as file:\n",
    "    train_labels_list = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CTDataset(train_array_list, train_labels_list)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 128 * 128, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 128 * 128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available and if not, default to CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {device}\")\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNNClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "output_list = []\n",
    "num_epochs = 10 \n",
    "for epoch in range(num_epochs):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for images, labels in data_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        images = images[:, np.newaxis, :, :]\n",
    "        images = images.float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        output_list.append(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)  # Get the predicted classes\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Convert to numpy arrays for use with sklearn\n",
    "    true_labels = np.array(true_labels)\n",
    "    pred_labels = np.array(pred_labels)\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    accuracy_list.append(accuracy)\n",
    "    loss_list.append(loss.item())\n",
    "    # Print the loss after each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "x_list = [x for x in range(num_epochs)]\n",
    "\n",
    "plt.subplot(2, 1, 1)  # (rows, columns, subplot index)\n",
    "plt.plot(x_list, loss_list)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss over epochs')\n",
    "\n",
    "# Create the second subplot\n",
    "plt.subplot(2, 1, 2)  # (rows, columns, subplot index)\n",
    "plt.plot(x_list, accuracy_list)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training accuracy over epochs')\n",
    "\n",
    "# Adjust the spacing between plots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, \"small_test_data.json\"), \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "with open(os.path.join(path, \"small_test_labels.json\"), \"r\") as file:\n",
    "    test_labels = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_test_contours = []\n",
    "skipped_test_cts = []\n",
    "total_test_arrays = []\n",
    "total_test_labels = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    try:\n",
    "        single_arrays = []\n",
    "        Dicom_path = test_data[i]\n",
    "        Dicom_reader = DicomReaderWriter(description='Examples', arg_max=True)\n",
    "        Dicom_reader.walk_through_folders(Dicom_path) \n",
    "        # all_rois = Dicom_reader.return_rois(print_rois=True) # Return a list of all rois present\n",
    "\n",
    "        right_side = lung_side(Dicom_path)\n",
    "        if right_side == False:\n",
    "            Contour_names = ['rlung'] \n",
    "            associations = [ROIAssociationClass('rlung', right_contours)]\n",
    "        else:\n",
    "            Contour_names = ['llung'] \n",
    "            associations = [ROIAssociationClass('llung', left_contours)]\n",
    "        Dicom_reader.set_contour_names_and_associations(contour_names=Contour_names, associations=associations)\n",
    "        indexes = Dicom_reader.which_indexes_have_all_rois()\n",
    "        if indexes != []:\n",
    "            pt_indx = indexes[-1]\n",
    "            Dicom_reader.set_index(pt_indx) \n",
    "            Dicom_reader.get_images_and_mask()  \n",
    "\n",
    "            image = Dicom_reader.ArrayDicom # image array\n",
    "            mask = Dicom_reader.mask # mask array\n",
    "\n",
    "            slice_locations = np.unique(np.where(mask != 0)[0]) # get indexes for where there is a contour present \n",
    "            slice_start = slice_locations[0] # first slice of contour \n",
    "            slice_end = slice_locations[len(slice_locations)-1] # last slice of contour\n",
    "\n",
    "            counter = 1\n",
    "\n",
    "            for img_arr, contour_arr in zip(image[slice_start:slice_end+1], mask[slice_start:slice_end+1]): \n",
    "                if counter % skip == 0: # if current slice is divisible by desired skip amount \n",
    "                    select = np.multiply(img_arr, contour_arr)\n",
    "                    single_arrays.append(select)\n",
    "                counter += 1\n",
    "\n",
    "            single_labels = [test_labels[i] for x in range(len(single_arrays))]\n",
    "            total_test_arrays = total_test_arrays + single_arrays\n",
    "            total_test_labels = total_test_labels + single_labels\n",
    "        else:\n",
    "            missed_test_contours.append(Dicom_path)\n",
    "    \n",
    "    except TypeError:\n",
    "        print(\"skip this dataset\")\n",
    "        skipped_test_cts.append(Dicom_path)\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(np_folder_path, \"ipsi_test_arrays.npy\"), total_test_arrays)\n",
    "\n",
    "with open(os.path.join(np_folder_path, \"ipsi_test_labels_np.json\"), \"w\") as file:\n",
    "    json.dump(total_test_labels, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array_list = np.load(os.path.join(np_folder_path, \"ipsi_test_arrays.npy\"), allow_pickle=True)\n",
    "\n",
    "with open(os.path.join(np_folder_path, \"ipsi_test_labels_np.json\"), \"r\") as file:\n",
    "    test_labels_list = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CTDataset(test_array_list, test_labels_list)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "outputs_list = []\n",
    "\n",
    "# Loop through the test data\n",
    "for inputs, labels in test_data_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    inputs = inputs[:, np.newaxis, :, :]\n",
    "    inputs = inputs.float()\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)  # Get the predicted classes\n",
    "\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "    pred_labels.extend(preds.cpu().numpy())\n",
    "    outputs_list.extend(outputs.detach().cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays for use with sklearn\n",
    "true_labels = np.array(true_labels)\n",
    "pred_labels = np.array(pred_labels)\n",
    "\n",
    "# Compute ROC AUC\n",
    "roc_auc = roc_auc_score(label_binarize(true_labels, classes=[0,1]),\n",
    "                        label_binarize(pred_labels, classes=[0,1]), \n",
    "                        average='macro')\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
